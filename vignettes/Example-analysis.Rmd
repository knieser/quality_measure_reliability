---
title: "Tutorial of QualityMeasure functions"
author: "Kenneth Nieser"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial of QualityMeasure functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=8, 
  fig.height=6
)
library(knitr)

```

This vignette shows how to use the functions in `QualityMeasure` to analyze quality measure performance and reliability. In the first half, we demonstrate how to use functions for measures without risk-adjustment. In the second half, we show how to incorporate risk-adjustment into the analyses.

```{r setup}
library(devtools)
load_all()
library(QualityMeasure)
```

------------------------------------------------------------------------

# Example #1: Measures without risk-adjustment

## Simulate data

We'll simulate some data to use for the example analyses in this section.

```{r}
set.seed(123)

# number of accountable entities
n.entity = 100  

# average number of patients/cases per accountable entity
n.pts = 50

# parameters of the Beta distribution
alpha = 1
beta = 9

# sample the number of patients/cases per accountable entity
n = rpois(n.entity, n.pts) 
total.n = sum(n)

# sample the true performance for each entity
p = rbeta(n.entity, alpha, beta) 

# make sample data
entity = rep(1:n.entity, times = n)
y = rbinom(total.n, 1, rep(p, times = n))
df1 = data.frame(entity = entity, y = y)
```

```{r, echo = FALSE, results = 'asis'}
knitr::kable(head(df1, 10), caption = 'Simulated data 1')
```

<br> <br>

## Provider profiling analyses

The `profiling_analysis()` function can be used to obtain some basic provider profiling results.

This analysis generates confidence intervals for entity-level performance using a parametric bootstrapping method. The number of bootstraps and the number of cores to use for parallel processing can be adjusted with the `controlPerf()` function within `profiling_analysis()` as shown below.

```{r}
# adjust number of bootstraps and cores for parallel processing.
n.boots = 1000
n.cores = 5

# run profiling analysis for the first dataset without risk-adjustment
profiling.results <- profiling_analysis(df = df1, ctrPerf = controlPerf(n.boots = n.boots, n.cores = n.cores))
perf.results <- profiling.results$perf.results
```

The estimated marginal probability of the outcome is `r round(profiling.results$marg.p, 2)`. Recall that the true marginal probability according to our data-generation process is `r round(alpha/(alpha + beta), 2)`.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(profiling.results$perf.summary, caption = 'Performance summary statistics across entities')
```

### Number of observations per entity

```{r}
plotN(perf.results$n)
```

### Unadjusted rates across entities

```{r}
# Unadjusted performance
plotPerformance(df = perf.results)
```

Categorization of entities according to whether rates are lower, higher, or no different from the overall, unadjusted average can be found in the `perf.results` output from `profiling_analysis()` function.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(table(perf.results$category.oe), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on their outcome rates')
```


### Plot of ORs comparing each entity with the "average" entity

Another way to assess whether entities have measure performance that differs from an "average" entity is to examine the predicted random intercept values.

Entities highlighted in red have estimated ORs that are statistically different from 1.0 at alpha = 0.05.

```{r}
plotPerformance(df = perf.results, plot.type = 'OR')
```

There are `r sum(perf.results$intercept.sig==1)` entities with outcome odds that are significantly different from the average entity performance.

<br> <br>

## Beta-binomial-based reliability estimates

You can calculate the Beta-Binomial estimates by using the `calcBetaBin()` function. Note that these estimates do not account for any risk-adjustment.

```{r}
BB.results <- calcBetaBin(df = df1)
```

The estimated alpha is `r round(BB.results$alpha, 3)` and beta is `r round(BB.results$beta, 3)`. Recall that the true values from the simulation are alpha = `r alpha` and beta = `r beta`.

Reliability summary statistics:

```{r, echo = FALSE}
summary(BB.results$est.BB)
```

If you have data that is already aggregated by entity, you can calculate Beta-Binomial estimates as follows:

```{r}
# Aggregated data
df.agg <- data.frame(n = aggregate(y ~ entity, data = df1, length)$y,
                     x = aggregate(y ~ entity, data = df1, sum)$y)


BB.agg.results <- calcBetaBin(df = df.agg, df.aggregate = T, n = 'n', x = 'x')
```

The estimated alpha is `r round(BB.agg.results$alpha, 3)` and beta is `r round(BB.agg.results$beta, 3)`. These estimates match our results from analyzing the data before aggregation.

<br> <br>

## Calculate reliability from all methods

If you would like to calculate reliability estimates from all the available methods in this package, you can use the `calcReliability()` function.

The number of resamples used to calculate the permutation split-sample reliability estimate can be adjusted using the `controlRel()` function within `calcReliability()` as shown below.

```{r}
# number of resamples to use for the permutation split-sample reliability estimate
n.resamples = 200

rel.out <- calcReliability(df = df1, ctrPerf = controlPerf(n.cores = n.cores), ctrRel = controlRel(n.resamples = n.resamples))
```

```{r, echo = FALSE, results = 'asis'}
rel.results <- rel.out$rel.results
rel.results.sub <- rel.results[,c('method', 'reliability', 'reliability_min', 'reliability_max')]
rel.results.sub$reliability <- round(rel.results.sub$reliability, 3)
rel.results.sub$reliability_min <- round(rel.results.sub$reliability_min, 3)
rel.results.sub$reliability_max <- round(rel.results.sub$reliability_max, 3)
names(rel.results.sub) <- c('Method', 'Reliability', 'Min Reliability', 'Max Reliability')

knitr::kable(rel.results.sub, caption = 'Reliability estimates')
```

```{r}
plotSNRReliability(rel.out)
```

<br><br>

------------------------------------------------------------------------

<br><br>

# Example #2: Measures with risk-adjustment

In this next part of the tutorial, we will work with an example measure where risk-adjustment is required.

## Simulate data

We can use the built-in function `simulateData()` to simulate data from a hierarchical logistic regression model with covariates for risk-adjustment. The simulated data will include a continuous covariate `x1` which is sampled from a standard Normal distribution.

```{r}
# number of accountable entities
n.entity = 100  

# average number of patients/cases per accountable entity
avg.n = 50

# marginal probability of the outcome
marg.p = .1 

# reliability for entity with an average number of patients
r = .6

# implied between-entity variance
var.btwn = r/(1 - r) * (1/(avg.n*marg.p*(1 - marg.p)))

mu = log(marg.p / (1 - marg.p))
tau = c(mu, sqrt(var.btwn))

# parameter for risk-adjustment model (i.e., coefficient for x1)
theta = log(1.5)

df2 <- simulateData(n.entity = n.entity, avg.n = avg.n, tau = tau, theta = theta)

```

```{r, echo = FALSE, results = 'asis'}
knitr::kable(head(df2, 10), caption = 'Simulated data with a covariate')
```
<br><br>

## Fit risk-adjustment model to the data

To incorporate risk-adjustment, we specify a model to use for risk adjustment:

```{r}
model = 'y ~ x1 + (1 | entity)'
```

This is a model that adjusts for `x1` and includes a random intercept for `entity`.

### Estimates of model parameters

```{r}
model.perf <- model_performance(df = df2, model = model)
plotEstimates(model.perf$model.results)
```

The estimated value of the regression coefficient, after exponentiating is, `r round(model.perf$model.results$est,2)` with a 95% confidence interval ranging from `r round(model.perf$model.results$lb,2)` to `r round(model.perf$model.results$ub,2)`. Recall that the true value from the simulation is `r exp(theta)`.

The estimated between-entity variance is `r round(lme4::VarCorr(model.perf$fit)[["entity"]][1,1], 3)`. Recall that the true value from the simulation is `r round(var.btwn, 3)`.

### Discrimination

The estimated c-statistic is `r round(model.perf$c.statistic,2)`, and below is a plot of the distributions of predicted probabilities separately for observations with and without the outcome occurring.

```{r}
plotPredictedDistribution(df = model.perf$df)
```

### Calibration

Below is a plot of the observed rate of the outcome for quantiles of the predicted probabilities from the model. The number of quantiles can be specified through the `quantiles` argument of the `plotCalibration()` function.

```{r}
plotCalibration(df = model.perf$df, quantiles = 10)
```
<br><br>

## Provider profiling analyses

```{r}
# run profiling analysis for the first dataset without risk-adjustment
profiling.results2 <- profiling_analysis(df = df2, model = model, ctrPerf = controlPerf(n.boots = n.boots, n.cores = n.cores))
perf.results2 <- profiling.results2$perf.results
```

The estimated marginal probability of the outcome is `r round(profiling.results2$marg.p, 2)`. Recall that the true value of the marginal probability from the simulation is `r marg.p`.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(profiling.results2$perf.summary, caption = 'Performance summary statistics across entities')
```

### Number of observations per entity

```{r}
plotN(perf.results2$n)
```

### Unadjusted rates across entities

```{r}
plotPerformance(df = perf.results2)
```

### OE-risk-standardized rates across entities

```{r}
plotPerformance(df = perf.results2, plot.y = 'oe')
```

Categorization of entities according to whether their risk-standardized rates are lower, higher, or no different from the overall, unadjusted average can be found in the `perf.results` output from `profiling_analysis()` function.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(table(perf.results2$category.oe), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on OE-risk-standardized rates')
```



### PE-risk-standardized rates across entities

```{r}
plotPerformance(df = perf.results2, plot.y = 'pe')
```
```{r, echo = FALSE, results = 'asis'}
knitr::kable(table(perf.results2$category.pe), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on PE-risk-standardized rates')
```


### Plot of ORs comparing each entity with the "average" entity

Another way to assess whether entities have measure performance that differs from an "average" entity is to examine the predicted random intercept values.

Entities highlighted in red have estimated ORs that are statistically different from 1.0 at alpha = 0.05.

```{r}
plotPerformance(df = perf.results2, plot.type = 'OR')
```
There are `r sum(perf.results2$intercept.sig==1)` entities with outcome adjusted odds that are significantly different from the average entity performance.

<br> <br>

## Calculate reliability from all methods

Again, to calculate reliability estimates from all the available methods in this package, we use the `calcReliability()` function. This time we specify the risk-adjustment model.

```{r}
rel.out2 <- calcReliability(df = df2, model = model, ctrPerf = controlPerf(n.cores = n.cores), ctrRel = controlRel(n.resamples = n.resamples))
```

```{r, echo = FALSE, results = 'asis'}
rel.results2 <- rel.out2$rel.results
rel.results.sub2 <- rel.results2[,c('method', 'reliability', 'reliability_min', 'reliability_max')]
rel.results.sub2$reliability <- round(rel.results.sub2$reliability, 3)
rel.results.sub2$reliability_min <- round(rel.results.sub2$reliability_min, 3)
rel.results.sub2$reliability_max <- round(rel.results.sub2$reliability_max, 3)
names(rel.results.sub2) <- c('Method', 'Reliability', 'Min Reliability', 'Max Reliability')

knitr::kable(rel.results.sub2, caption = 'Reliability estimates')
```

```{r}
plotSNRReliability(rel.out2)
```
