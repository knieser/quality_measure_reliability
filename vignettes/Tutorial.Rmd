---
title: "Tutorial of QualityMeasure functions"
author: "Kenneth Nieser"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial of QualityMeasure functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=8, 
  fig.height=6
)
library(knitr)

```

This vignette shows how to use the functions in `QualityMeasure` to analyze quality measure performance and reliability. In the first half, we demonstrate how to use functions for measures without risk-adjustment. In the second half, we show how to incorporate risk-adjustment into the analyses.

```{r setup}
library(QualityMeasure)
```

------------------------------------------------------------------------

# Example #1: Measures without risk-adjustment

## Simulate data

We'll simulate some data to use for the example analyses in this section.

```{r}
set.seed(123)

# number of accountable entities
n.entity = 100  

# average number of patients/cases per accountable entity
n.obs = 50

# marginal probability of the outcome
mu = .2

# reliability
r = .7

# make sample data
df1 <- simulateData(n.entity = n.entity, n.obs = n.obs, mu = mu, r = r)
```

<br> <br>

## Provider profiling analyses

The `profiling_analysis()` function can be used to obtain some basic provider profiling results, including confidence intervals for facility-level rates.

```{r}
# run profiling analysis for the first dataset without risk-adjustment
profiling.results <- profiling_analysis(df = df1)
perf.results <- profiling.results$perf.results
```

The estimated marginal probability of the outcome is `r round(profiling.results$marg.p, 2)`. Recall that the true marginal probability according to our data-generation process is `r round(mu, 2)`.

```{r, echo = FALSE, results = 'asis'}
rate.summary = summary(perf.results$p)
perf.summary = cbind(Method = c('Unadjusted'), round(rate.summary, 3))
perf.summary = as.data.frame(perf.summary)
knitr::kable(perf.summary, caption = 'Performance summary statistics across entities')
```

### Number of observations per entity

```{r}
plotN(perf.results$n)
```

### Unadjusted rates across entities

```{r}
# Unadjusted performance
plotPerformance(df = perf.results)
```

Categorization of entities according to whether rates are lower, higher, or no different from the overall, unadjusted average can be found in the `perf.results` output from `profiling_analysis()` function.

```{r, results = 'asis'}
knitr::kable(table(perf.results$category.p), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on their outcome rates')
```


### Plot of ORs comparing each entity with the "average" entity

Another way to assess whether entities have measure performance that differs from an "average" entity is to examine the predicted random intercept values.

Entities highlighted in red have estimated ORs that are statistically different from 1.0 at alpha = 0.05.

```{r}
plotPerformance(df = perf.results, plot.type = 'OR')
```

There are `r sum(perf.results$intercept.sig==1)` entities with outcome odds that are significantly different from the average entity performance.

<br> <br>

## Beta-binomial-based reliability estimates

You can calculate the Beta-Binomial estimates by using the `calcBetaBin()` function. Note that these estimates do not account for any risk-adjustment.

```{r}
BB.results <- calcBetaBin(df = df1)
```

The estimated alpha is `r round(BB.results$alpha, 3)` and beta is `r round(BB.results$beta, 3)`. 

Reliability summary statistics:

```{r, echo = FALSE}
summary(BB.results$est.BB)
```

If you have data that is already aggregated by entity, you can calculate Beta-Binomial estimates as follows:

```{r}
# Aggregated data
df.agg <- data.frame(n = aggregate(y ~ entity, data = df1, length)$y,
                     x = aggregate(y ~ entity, data = df1, sum)$y)


BB.agg.results <- calcBetaBin(df = df.agg, df.aggregate = T, n = 'n', x = 'x')
```

The estimated alpha is `r round(BB.agg.results$alpha, 3)` and beta is `r round(BB.agg.results$beta, 3)`. These estimates match our results from analyzing the data before aggregation.

<br> <br>

## Calculate reliability from all methods

If you would like to calculate reliability estimates from all methods, you can use the `calcReliability()` function.

The number of resamples used to calculate the permutation split-sample reliability estimate can be adjusted using the `controlRel()` function within `calcReliability()` as shown below.

```{r}
# number of resamples to use for the permutation split-sample reliability estimate
n.resamples = 200

rel.out <- calcReliability(df = df1, ctrRel = controlRel(n.resamples = n.resamples))
```

```{r, echo = FALSE, results = 'asis'}
rel.results <- rel.out$rel.results
rel.results.sub <- rel.results[,c('method', 'reliability', 'reliability_min', 'reliability_max')]
rel.results.sub$reliability <- round(rel.results.sub$reliability, 3)
rel.results.sub$reliability_min <- round(rel.results.sub$reliability_min, 3)
rel.results.sub$reliability_max <- round(rel.results.sub$reliability_max, 3)
names(rel.results.sub) <- c('Method', 'Reliability', 'Min Reliability', 'Max Reliability')

knitr::kable(rel.results.sub, caption = 'Reliability estimates')
```

```{r}
plotReliability(rel.out)
```

<br><br>

------------------------------------------------------------------------

<br><br>

# Example #2: Measures with risk-adjustment

In this next part of the tutorial, we will work with an example measure where risk-adjustment is required.

## Simulate data

We can use the built-in function `simulateData()` to simulate data from a hierarchical logistic regression model with covariates for risk-adjustment. The simulated data will include a continuous covariate `x1` which is sampled from a standard Normal distribution.

```{r}
# number of accountable entities
n.entity = 100  

# average number of patients/cases per accountable entity
n.obs = 50

# marginal probability of the outcome
mu = .2 

# reliability for entity with an median number of patients
r = .6

# parameter for risk-adjustment model---coefficient for x1 which is simulated from a standard Normal
beta1 = log(1.5)

# make sample data
df2 <- simulateData(n.entity = n.entity, n.obs = n.obs, mu = mu, r = r, beta1 = beta1)

```

```{r, echo = FALSE, results = 'asis'}
knitr::kable(head(df2, 10), caption = 'Simulated data with a covariate')
```
<br><br>

## Fit risk-adjustment model to the data

To incorporate risk-adjustment, we specify a model to use for risk adjustment:

```{r}
model = 'y ~ x1 + (1 | entity)'
```

This is a model that adjusts for `x1` and includes a random intercept for `entity`.

### Estimates of model parameters

```{r}
model.perf <- model_performance(df = df2, model = model)
plotEstimates(model.perf)
```

The estimated value of the regression coefficient, after exponentiating is, `r round(model.perf$model.results$est,2)` with a 95% confidence interval ranging from `r round(model.perf$model.results$lb,2)` to `r round(model.perf$model.results$ub,2)`. 

The estimated between-entity variance is `r round(lme4::VarCorr(model.perf$fit)[["entity"]][1,1], 3)`. 

### Discrimination

The estimated c-statistic is `r round(model.perf$c.statistic,2)`, and below is a plot of the distributions of predicted probabilities separately for observations with and without the outcome occurring.

```{r}
plotPredictedDistribution(model.perf)
```

### Calibration

Below is a plot of the observed rate of the outcome for quantiles of the predicted probabilities from the model. The number of quantiles can be specified through the `quantiles` argument of the `plotCalibration()` function.

```{r}
plotCalibration(model.perf, quantiles = 10)
```
<br><br>

## Provider profiling analyses

The `profiling_analysis()` function can be used to obtain some basic provider profiling results.

This analysis generates confidence intervals for entity-level performance. Confidence intervals for the observed-to-expected (O/E) standardized rates are Wilson score intervals, while the confidence intervals for the predicted-to-expected (P/E) standardized rates use a parametric bootstrapping method. The number of bootstraps and the number of cores to use for parallel processing can be adjusted with the `controlPerf()` function within `profiling_analysis()` as shown below.

```{r}
# adjust number of bootstraps and cores for parallel processing.
n.boots = 100
n.cores = 2

# run profiling analysis for the first dataset without risk-adjustment
profiling.results2 <- profiling_analysis(df = df2, model = model, ctrPerf = controlPerf(n.boots = n.boots, n.cores = n.cores))
perf.results2 <- profiling.results2$perf.results
```

The estimated marginal probability of the outcome is `r round(profiling.results2$marg.p, 2)`. Recall that the true value of the marginal probability from the simulation is `r mu`.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(profiling.results2$perf.summary, caption = 'Performance summary statistics across entities')
```

### Number of observations per entity

```{r}
plotN(perf.results2$n)
```

### Unadjusted rates across entities

```{r}
plotPerformance(df = perf.results2)
```

### OE-risk-standardized rates across entities

```{r}
plotPerformance(df = perf.results2, plot.y = 'oe')
```

Categorization of entities according to whether their risk-standardized rates are lower, higher, or no different from the overall, unadjusted average can be found in the `perf.results` output from `profiling_analysis()` function.

```{r, echo = FALSE, results = 'asis'}
knitr::kable(table(perf.results2$category.oe), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on OE-risk-standardized rates')
```



### PE-risk-standardized rates across entities

```{r}
plotPerformance(df = perf.results2, plot.y = 'pe')
```
```{r, echo = FALSE, results = 'asis'}
knitr::kable(table(perf.results2$category.pe), col.names = c('Category', 'Number of entities'), caption = 'Categorization of entities based on PE-risk-standardized rates')
```


### Plot of ORs comparing each entity with the "average" entity

Another way to assess whether entities have measure performance that differs from an "average" entity is to examine the predicted random intercept values.

Entities highlighted in red have estimated ORs that are statistically different from 1.0 at alpha = 0.05.

```{r}
plotPerformance(df = perf.results2, plot.type = 'OR')
```
There are `r sum(perf.results2$intercept.sig==1)` entities with outcome adjusted odds that are significantly different from the average entity performance.

<br> <br>

## Calculate reliability from all methods

Again, to calculate reliability estimates from all methods, we use the `calcReliability()` function. This time we specify the risk-adjustment model.

```{r}
rel.out2 <- calcReliability(df = df2, model = model, ctrPerf = controlPerf(n.cores = n.cores), ctrRel = controlRel(n.resamples = n.resamples))
```

```{r, echo = FALSE, results = 'asis'}
rel.results2 <- rel.out2$rel.results
rel.results.sub2 <- rel.results2[,c('method', 'reliability', 'reliability_min', 'reliability_max')]
rel.results.sub2$reliability <- round(rel.results.sub2$reliability, 3)
rel.results.sub2$reliability_min <- round(rel.results.sub2$reliability_min, 3)
rel.results.sub2$reliability_max <- round(rel.results.sub2$reliability_max, 3)
names(rel.results.sub2) <- c('Method', 'Reliability', 'Min Reliability', 'Max Reliability')

knitr::kable(rel.results.sub2, caption = 'Reliability estimates')
```

```{r}
plotReliability(rel.out2)
```
